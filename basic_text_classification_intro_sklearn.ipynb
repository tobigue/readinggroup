{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very basic introduction to text classification using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals, division\n",
    "import random\n",
    "import re\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting some data\n",
    "\n",
    "The toy example we are going to use for this introduction is subjects of emails from the corpora mailing list, which are categorized in job opening announcements vs. other emails on the mailing list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"job\", \"Postdoc in NLP and Machine Learning at the University College London\"),\n",
    "    (\"job\", \"Senior Research Associate Post in Language Development, Lancaster University\"),\n",
    "    (\"job\", \"Postdoc position in computational linguistics (reminder!)\"),\n",
    "    (\"job\", \"Call for applicants : PhD on incremental text mining\"),\n",
    "    (\"job\", \"Job Opening: Post-doctoral research fellow (Osnabr√ºck University; DFG-funded fixed term contract, 3 years)\"),\n",
    "    (\"job\", \"second announcement: postdoc position in computational linguistics at Duesseldorf, Germany\"),\n",
    "    (\"job\", \"A job opportunity at Lancaster: Java programmer\"),\n",
    "    (\"job\", \"looking for a Farsi linguist\"),\n",
    "    (\"job\", \"Lecturer in Data Engineering at Lancaster University\"),\n",
    "    (\"job\", \"Job: Research or Lead Scientist - automatic language processing, text mining, and information management\"),\n",
    "\n",
    "    (\"other\", \"New Journal: Language and Law / Linguagem e Direito\"),\n",
    "    (\"other\", \"Deadline extension: Challenges in the management of large corpora\"),\n",
    "    (\"other\", \"Call for Participation 5th Lisbon Machine Learning School\"),\n",
    "    (\"other\", \"Final CFP: NAACL 2016 Workshop on Metaphor in NLP\"),\n",
    "    (\"other\", \"Announcement: Course in Statistics for PhD Students\"),\n",
    "    (\"other\", \"Reminder: English Profile Seminar\"),\n",
    "    (\"other\", \"Beth PhD Thesis Prize\"),\n",
    "    (\"other\", \"Looking for Transliteration Data\"),\n",
    "    (\"other\", \"Participants needed for a research web study on interactive virtual characters\"),\n",
    "    (\"other\", \"LINGUIST List birthday and new service GeoLing\"),\n",
    "]\n",
    "\n",
    "random.shuffle(training_data)\n",
    "labels, texts = zip(*training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & feature extraction\n",
    "\n",
    "As a first step we transform our each of our texts to a set of features that will be used by the classifier.\n",
    "\n",
    "We are going to use simple bag of word features for this demo, but you can create any features you like.\n",
    "\n",
    "Note that sklearn also comes with a variety of feature extraction methods, so for basic things there is not even the need to \"do it yourself\" if you don't want to.\n",
    "For more info see: [Feature extraction](http://scikit-learn.org/stable/modules/feature_extraction.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 1,\n",
       " 'birthday': 1,\n",
       " 'geoling': 1,\n",
       " 'linguist': 1,\n",
       " 'list': 1,\n",
       " 'new': 1,\n",
       " 'service': 1}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = re.compile(\"[^\\W_]+\", re.UNICODE)\n",
    "\n",
    "def extract_features(text):\n",
    "    tokens = tokenizer.findall(text)\n",
    "    # we are going to use simple bag of word features for this demo\n",
    "    features = {token.lower(): 1 for token in tokens}\n",
    "    return features\n",
    "\n",
    "features = [extract_features(text) for text in texts]\n",
    "features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the features\n",
    "\n",
    "Next we need to transform our data to a format that sklearn estimators can work with.\n",
    "\n",
    "As we stored our features in dictionaries, we are going to use the [DictVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 106)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vectorizer = DictVectorizer()\n",
    "\n",
    "# we create the feature matrix by fitting the vectorizer\n",
    "# (building a mapping from feature names to vector indices)\n",
    "# and transforming the given data\n",
    "X = vectorizer.fit_transform(features)\n",
    "\n",
    "X.shape  # number examples x number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we had more data we could try to count the occurences of tokens in our texts and weight the features according to TF-IDF using a [TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer)\n",
    "* If we had too many features to efficiently train a classifier, we could use a [FeatureHasher](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher)\n",
    "* Vectorizers, Transformers and Classifiers can be combined into [Pipelines](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a classifier\n",
    "\n",
    "\n",
    "Let's train a simple Naive Bayes classifier and test it with two examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Postdoc/Programmer positions in Biomedical NLP', 'job'),\n",
      " ('1st CfP: 2nd Workshop on NLP for Translation Memories at LREC 2016',\n",
      "  'other')]\n"
     ]
    }
   ],
   "source": [
    "y = labels\n",
    "\n",
    "# train a classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "\n",
    "examples = [\n",
    "    \"Postdoc/Programmer positions in Biomedical NLP\",\n",
    "    \"1st CfP: 2nd Workshop on NLP for Translation Memories at LREC 2016\"\n",
    "]\n",
    "\n",
    "# it is important to use same vectorizer we fitted before (don't fit again!),\n",
    "# to transform the test data, as we need the same vector dimensions for\n",
    "# training and test data\n",
    "X_test = vectorizer.transform([extract_features(text) for text in examples])\n",
    "\n",
    "# let the model predict the categories of the examples\n",
    "predictions = clf.predict(X_test)\n",
    "pprint(list(zip(examples, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*So far so good - we have a model that can make predictions!*\n",
    "\n",
    "* If you want to [persist your model](http://scikit-learn.org/stable/modules/model_persistence.html), look into joblib and don't forget to persist the vectorizer that goes with your model as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a cross-validation evaluation of different classifiers\n",
    "\n",
    "Sklearn offers inbuild methods to to train/test splits of the data and do cross validations.\n",
    "\n",
    "As all classifiers in sklearn have the same basic interface, it is easy to compare a few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DummyClassifier(constant=None, random_state=None, strategy='uniform')\n",
      "Avg. accuracy: 0.2500 (+/- 0.1250)\n",
      "\n",
      " BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Avg. accuracy: 0.9000 (+/- 0.1000)\n",
      "\n",
      " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Avg. accuracy: 0.6000 (+/- 0.1000)\n",
      "\n",
      " Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
      "      n_iter=100, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
      "      verbose=0, warm_start=False)\n",
      "Avg. accuracy: 0.5500 (+/- 0.1346)\n",
      "\n",
      " LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "Avg. accuracy: 0.6500 (+/- 0.1146)\n",
      "\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params={'warn_on_equidistant': False}, n_jobs=1,\n",
      "           n_neighbors=5, p=2, weights='distance')\n",
      "Avg. accuracy: 0.6000 (+/- 0.1500)\n",
      "\n",
      " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Avg. accuracy: 0.6500 (+/- 0.1146)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifiers_to_test = [\n",
    "    DummyClassifier(strategy='uniform'),\n",
    "    BernoulliNB(),\n",
    "    LogisticRegression(),\n",
    "    Perceptron(n_iter=100),\n",
    "    LinearSVC(),\n",
    "    KNeighborsClassifier(warn_on_equidistant=False, weights=\"distance\"),\n",
    "    RandomForestClassifier()\n",
    "]\n",
    "\n",
    "from sklearn import cross_validation\n",
    "for clf in classifiers_to_test:\n",
    "    print(\"\\n\", clf)\n",
    "    scores = cross_validation.cross_val_score(clf, X, y, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "    print(\"Avg. accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() / 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize hyperparameters\n",
    "\n",
    "Let's see if we can get some better results for support vector machines if we tune the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.75\n",
      "Best params: {'C': 0.1, 'loss': 'hinge', 'fit_intercept': True, 'max_iter': 5}\n",
      "Best model: LinearSVC(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='hinge', max_iter=5, multi_class='ovr',\n",
      "     penalty='l2', random_state=None, tol=0.0001, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "parameters = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'loss': [\"hinge\", \"squared_hinge\"],\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10, 100, 1000],\n",
    "    'max_iter': [5, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "grid = GridSearchCV(clf, parameters, n_jobs=-1, cv=10, scoring=\"accuracy\")\n",
    "grid.fit(X, y)\n",
    "print(\"Best score:\", grid.best_score_)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best model:\", grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*An improvement of 0.1, but still not as good as the Naive Bayes classifier on this tiny dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test performance on hold out test set\n",
    "\n",
    "In the end, let's test our best model with optimized hyperparameters against some hold-out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        job       1.00      1.00      1.00         3\n",
      "      other       1.00      1.00      1.00         3\n",
      "\n",
      "avg / total       1.00      1.00      1.00         6\n",
      "\n",
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    (\"job\", \"second announcement: postdoc position in computational linguistics at Duesseldorf, Germany\"),\n",
    "    (\"job\", \"Job: Research Associate\"),\n",
    "    (\"job\", \"10 PhD places in Data Science at the University of Edinburgh\"),\n",
    "\n",
    "    (\"other\", \"Looking for available similar sentences corpora\"),\n",
    "    (\"other\", \"PR: The Resource Management Agency (RMA) adopts the ISLRN\"),\n",
    "    (\"other\", \"Announcement: Translating and the Computer 37\"),\n",
    "]\n",
    "# transform data\n",
    "test_labels, test_texts = zip(*test_data)\n",
    "test_features = [extract_features(text) for text in test_texts]\n",
    "X_test = vectorizer.transform(test_features)\n",
    "\n",
    "# train classifier and make predictions\n",
    "clf = BernoulliNB().fit(X, y)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "from sklearn import metrics\n",
    "y_test = np.array(test_labels)\n",
    "print(metrics.classification_report(y_test, test_predictions))\n",
    "print(\"Accuracy: %0.4f\" % metrics.accuracy_score(y_test, test_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
